# THIS FILE HAS BEEN AUTOMATICALLY GENERATED BY WHALE
# Only modify it using the whale command line
# Use a .gitlab-ci-*.yml file to exclude non-generated content.
# See whale --help for further information
---

image: 730164216233.dkr.ecr.eu-central-1.amazonaws.com/aws-docker:v6.14.0

stages:
- build
- before_test
- test
- after_test
- release
- deploy
- after_deploy
- mr_bot

before_script: &before_script
- AWS_ACCOUNT_NAME=kaeuferportal
- source /usr/local/bin/aws-assume-role
- "$(aws ecr get-login --no-include-email --region eu-central-1 --registry-ids 730164216233)"

variables:
  BASE_IMAGE_NAME: 730164216233.dkr.ecr.eu-central-1.amazonaws.com/aroundhome/partner-profile-ui:base-$CI_COMMIT_SHORT_SHA
  BASE_IMAGE_TAG: base-$CI_COMMIT_SHORT_SHA
  CACHE_IMAGE_NAME: 730164216233.dkr.ecr.eu-central-1.amazonaws.com/aroundhome/partner-profile-ui:cache-$CI_COMMIT_REF_SLUG
  CACHE_IMAGE_TAG: cache-$CI_COMMIT_REF_SLUG
  IMAGE_PATH: 730164216233.dkr.ecr.eu-central-1.amazonaws.com/aroundhome/partner-profile-ui
  DEV_IMAGE_NAME: 730164216233.dkr.ecr.eu-central-1.amazonaws.com/aroundhome/partner-profile-ui:development-$CI_COMMIT_REF_SLUG

include:
- ".gitlab-ci-tests.yml"

build:
  stage: build
  script:
  - docker pull ${DEV_IMAGE_NAME} || docker pull ${IMAGE_PATH}:development-staging || true
  - docker pull ${CACHE_IMAGE_NAME} || docker pull ${IMAGE_PATH}:release-candidate-staging || true
  - docker build -f Dockerfile -t ${DEV_IMAGE_NAME} --target development --cache-from ${DEV_IMAGE_NAME} --cache-from ${IMAGE_PATH}:development-staging .
  - "ERROR_COUNT=$(docker inspect ${DEV_IMAGE_NAME} | jq '.[0] | .Config.Cmd | join(\" \")' | grep -c \"/bin/sh -c \\#(nop)\") || true"
  - 'if [ "$ERROR_COUNT" -ge 1 ]; then docker build -f Dockerfile -t ${DEV_IMAGE_NAME} --target development --no-cache .; fi'
  - 'docker build -f Dockerfile -t ${BASE_IMAGE_NAME} --build-arg="SENTRY_RELEASE=${CI_COMMIT_SHORT_SHA}" --cache-from ${CACHE_IMAGE_NAME} --cache-from ${DEV_IMAGE_NAME} --cache-from ${IMAGE_PATH}:release-candidate-staging .'
  - "ERROR_COUNT=$(docker inspect ${BASE_IMAGE_NAME} | jq '.[0] | .Config.Cmd | join(\" \")' | grep -c \"/bin/sh -c \\#(nop)\") || true"
  - 'if [ "$ERROR_COUNT" -ge 1 ]; then docker build -f Dockerfile -t ${BASE_IMAGE_NAME} --build-arg="SENTRY_RELEASE=${CI_COMMIT_SHORT_SHA}" --no-cache .; fi'
  - docker push ${DEV_IMAGE_NAME}
  - docker push ${BASE_IMAGE_NAME}
  - tag-image aroundhome/partner-profile-ui ${BASE_IMAGE_TAG} ${CACHE_IMAGE_TAG}
  retry:
    max: 2
    when:
    - runner_system_failure

whale validate:
  stage: test
  image: 730164216233.dkr.ecr.eu-central-1.amazonaws.com/aroundhome/whale:5.9.4
  before_script:
  - cp -r ./ /app/
  - cd /app
  script:
  - bundle exec whale --validate
  dependencies: []
  retry:
    max: 2
    when:
    - runner_system_failure

pin_chart_version_production:
  stage: release
  retry:
    max: 2
    when:
    - runner_system_failure
  dependencies: []
  only:
  - production
  before_script:
  - export DEPLOY_ENV=staging
  - 'if [ "$CI_COMMIT_REF_SLUG" = "production" ]; then export DEPLOY_ENV=production; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "staging" ]; then export DEPLOY_ENV=staging; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "services" ]; then export DEPLOY_ENV=services; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "development" ]; then export DEPLOY_ENV=development; fi'
  - AWS_ACCOUNT_NAME=$DEPLOY_ENV
  - source /usr/local/bin/aws-assume-role
  script:
  - mkdir chart_version
  - helm3 repo add --force-update partner-profile-ui-production s3://helmcharts-arh/partner-profile-ui/production
  - helm3 show chart partner-profile-ui-production/partner-profile-ui > chart_version_info.yml
  - "yq -e eval '.version' chart_version_info.yml > chart_version/version"
  - cat chart_version/version
  artifacts:
    when: on_success
    expire_in: 1 month
    untracked: true
    paths:
    - chart_version/

pin_chart_version_staging:
  stage: release
  retry:
    max: 2
    when:
    - runner_system_failure
  dependencies: []
  except:
  - production
  before_script:
  - export DEPLOY_ENV=staging
  - 'if [ "$CI_COMMIT_REF_SLUG" = "production" ]; then export DEPLOY_ENV=production; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "staging" ]; then export DEPLOY_ENV=staging; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "services" ]; then export DEPLOY_ENV=services; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "development" ]; then export DEPLOY_ENV=development; fi'
  - AWS_ACCOUNT_NAME=$DEPLOY_ENV
  - source /usr/local/bin/aws-assume-role
  script:
  - mkdir chart_version
  - helm3 repo add --force-update partner-profile-ui-staging s3://helmcharts-arh/partner-profile-ui/staging
  - helm3 show chart partner-profile-ui-staging/partner-profile-ui > chart_version_info.yml
  - "yq -e eval '.version' chart_version_info.yml > chart_version/version"
  - cat chart_version/version
  artifacts:
    when: on_success
    expire_in: 1 month
    untracked: true
    paths:
    - chart_version/

release_image:
  stage: release
  retry:
    max: 2
    when:
    - runner_system_failure
  dependencies: []
  before_script:
  - export DEPLOY_ENV=staging
  - 'if [ "$CI_COMMIT_REF_SLUG" = "production" ]; then export DEPLOY_ENV=production; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "staging" ]; then export DEPLOY_ENV=staging; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "services" ]; then export DEPLOY_ENV=services; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "development" ]; then export DEPLOY_ENV=development; fi'
  - AWS_ACCOUNT_NAME=$DEPLOY_ENV
  - source /usr/local/bin/aws-assume-role
  - "$(aws ecr get-login --no-include-email --region eu-central-1 --registry-ids 730164216233)"
  script:
  - 'if [ "$DEPLOY_ENV" = "staging" ] ; then cp .env.staging .env.production; fi'
  - 'docker build --build-arg="BASE_IMAGE_NAME=${BASE_IMAGE_NAME}" --build-arg="SENTRY_RELEASE=${CI_COMMIT_SHORT_SHA}" -f Dockerfile.release -t ${IMAGE_PATH}:release-${CI_COMMIT_SHORT_SHA} .'
  - docker push ${IMAGE_PATH}:release-${CI_COMMIT_SHORT_SHA}

deploy_to_production:
  stage: deploy
  only:
  - production
  variables:
    GIT_STRATEGY: none
  when: manual
  allow_failure: false
  before_script:
  - 'if [ "$CI_COMMIT_REF_SLUG" = "production" ]; then export DEPLOY_TAG=latest; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "staging" ]; then export DEPLOY_TAG=staging; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "services" ]; then export DEPLOY_TAG=services; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "development" ]; then export DEPLOY_TAG=development; fi'
  script:
  - source /usr/local/bin/reset-role kaeuferportal
  - source /usr/local/bin/aws-assume-role
  - verify-image-tag aroundhome/partner-profile-ui ${DEPLOY_TAG} ${BASE_IMAGE_TAG}
  - verify-image-tag aroundhome/partner-profile-ui rollback-${DEPLOY_TAG} ${BASE_IMAGE_TAG}
  - tag-image aroundhome/partner-profile-ui release-${CI_COMMIT_SHORT_SHA} release-candidate-${DEPLOY_TAG}
  - source /usr/local/bin/reset-role production
  - source /usr/local/bin/aws-assume-role
  - CLUSTER_NAME=$PRODUCTION_CLUSTER_NAME
  - 'if [ "$CLUSTER_NAME" = "" ]; then echo "Cluster name not defined."; exit 1; fi'
  - aws eks --region eu-central-1 update-kubeconfig --name $CLUSTER_NAME
  - helm3 repo add --force-update partner-profile-ui-production s3://helmcharts-arh/partner-profile-ui/production || HELM_STATUS=error
  - 'helm3 upgrade --install partner-profile-ui partner-profile-ui-production/partner-profile-ui --namespace aroundhome --version $(cat chart_version/version) --set app.appVersion="${CI_COMMIT_SHORT_SHA}" --set app.image.tag="release-${CI_COMMIT_SHORT_SHA}" --atomic --timeout 300s || DEPLOY_STATUS=error'
  - source /usr/local/bin/reset-role kaeuferportal
  - source /usr/local/bin/aws-assume-role
  - 'if [ "$DEPLOY_STATUS" != "error" ] && [ "$HELM_STATUS" != "error" ]; then tag-image aroundhome/partner-profile-ui $DEPLOY_TAG rollback-$DEPLOY_TAG && tag-image aroundhome/partner-profile-ui release-candidate-$DEPLOY_TAG $DEPLOY_TAG; else tag-image aroundhome/partner-profile-ui release-candidate-$DEPLOY_TAG failed-release-${DEPLOY_TAG} && tag-image aroundhome/partner-profile-ui $DEPLOY_TAG release-candidate-$DEPLOY_TAG && echo "Deploy failed, all tags have been rolled back." && echo "The image tried to deploy has been tagged with: failed-release-${DEPLOY_TAG}" && exit 1; fi'
  dependencies:
  - pin_chart_version_production
  environment:
    name: production

deploy_to_staging:
  stage: deploy
  only:
  - staging
  variables:
    GIT_STRATEGY: none
  when: on_success
  allow_failure: false
  before_script:
  - 'if [ "$CI_COMMIT_REF_SLUG" = "production" ]; then export DEPLOY_TAG=latest; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "staging" ]; then export DEPLOY_TAG=staging; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "services" ]; then export DEPLOY_TAG=services; fi'
  - 'if [ "$CI_COMMIT_REF_SLUG" = "development" ]; then export DEPLOY_TAG=development; fi'
  script:
  - source /usr/local/bin/reset-role kaeuferportal
  - source /usr/local/bin/aws-assume-role
  - verify-image-tag aroundhome/partner-profile-ui ${DEPLOY_TAG} ${BASE_IMAGE_TAG}
  - verify-image-tag aroundhome/partner-profile-ui rollback-${DEPLOY_TAG} ${BASE_IMAGE_TAG}
  - tag-image aroundhome/partner-profile-ui release-${CI_COMMIT_SHORT_SHA} release-candidate-${DEPLOY_TAG}
  - source /usr/local/bin/reset-role staging
  - source /usr/local/bin/aws-assume-role
  - CLUSTER_NAME=$STAGING_CLUSTER_NAME
  - 'if [ "$CLUSTER_NAME" = "" ]; then echo "Cluster name not defined."; exit 1; fi'
  - aws eks --region eu-central-1 update-kubeconfig --name $CLUSTER_NAME
  - helm3 repo add --force-update partner-profile-ui-staging s3://helmcharts-arh/partner-profile-ui/staging || HELM_STATUS=error
  - 'helm3 upgrade --install partner-profile-ui partner-profile-ui-staging/partner-profile-ui --namespace aroundhome --version $(cat chart_version/version) --set app.appVersion="${CI_COMMIT_SHORT_SHA}" --set app.image.tag="release-${CI_COMMIT_SHORT_SHA}" --atomic --timeout 300s || DEPLOY_STATUS=error'
  - source /usr/local/bin/reset-role kaeuferportal
  - source /usr/local/bin/aws-assume-role
  - 'if [ "$DEPLOY_STATUS" != "error" ] && [ "$HELM_STATUS" != "error" ]; then tag-image aroundhome/partner-profile-ui $DEPLOY_TAG rollback-$DEPLOY_TAG && tag-image aroundhome/partner-profile-ui release-candidate-$DEPLOY_TAG $DEPLOY_TAG; else tag-image aroundhome/partner-profile-ui release-candidate-$DEPLOY_TAG failed-release-${DEPLOY_TAG} && tag-image aroundhome/partner-profile-ui $DEPLOY_TAG release-candidate-$DEPLOY_TAG && echo "Deploy failed, all tags have been rolled back." && echo "The image tried to deploy has been tagged with: failed-release-${DEPLOY_TAG}" && exit 1; fi'
  dependencies:
  - pin_chart_version_staging
  environment:
    name: staging
  retry:
    max: 2
    when:
    - runner_system_failure

deploy_review:
  stage: deploy
  only:
  - branches
  except:
  - production
  - staging
  variables:
    GIT_STRATEGY: none
  before_script:
  - AWS_ACCOUNT_NAME=staging
  - source /usr/local/bin/aws-assume-role
  - CLUSTER_NAME=$STAGING_CLUSTER_NAME
  - 'if [ "$CLUSTER_NAME" = "" ]; then echo "Cluster name not defined."; exit 1; fi'
  - aws eks --region eu-central-1 update-kubeconfig --name $CLUSTER_NAME
  script:
  - helm3 repo add --force-update partner-profile-ui-staging s3://helmcharts-arh/partner-profile-ui/staging
  - 'helm3 upgrade --install r57-$CI_COMMIT_REF_SLUG partner-profile-ui-staging/partner-profile-ui --namespace aroundhome --version $(cat chart_version/version) --set app.image.tag=release-${CI_COMMIT_SHORT_SHA} --set app.deploymentName=review-$(echo -n $CI_COMMIT_REF_SLUG $ | md5sum | cut -c1-7) --set app.appVersion=review --set "app.ingress.gateways={review-apps}" --set "app.service.hostname=partner-profile-ui-$CI_COMMIT_REF_SLUG.review-aroundhome" --set app.appServer.autoscaling.enabled=false --set app.appServer.disruptionBudget.enabled=false --set app.appServer.replicaCount=1'
  dependencies:
  - pin_chart_version_staging
  retry:
    max: 2
    when:
    - runner_system_failure
  environment:
    auto_stop_in: 2 weeks
    name: review/r57-$CI_COMMIT_REF_SLUG
    url: https://partner-profile-ui-$CI_COMMIT_REF_SLUG.review-aroundhome.aroundhome-staging.de/fachfirmen
    on_stop: stop_review_app
    deployment_tier: development

stop_review_app:
  stage: after_deploy
  when: manual
  only:
  - branches
  except:
  - production
  - staging
  variables:
    GIT_STRATEGY: none
  before_script:
  - AWS_ACCOUNT_NAME=staging
  - source /usr/local/bin/aws-assume-role
  - CLUSTER_NAME=$STAGING_CLUSTER_NAME
  - 'if [ "$CLUSTER_NAME" = "" ]; then echo "Cluster name not defined."; exit 1; fi'
  - aws eks --region eu-central-1 update-kubeconfig --name $CLUSTER_NAME
  script:
  - helm3 delete --namespace aroundhome r57-$CI_COMMIT_REF_SLUG
  dependencies: []
  environment:
    name: review/r57-$CI_COMMIT_REF_SLUG
    action: stop

staging_mr:
  stage: mr_bot
  image: 730164216233.dkr.ecr.eu-central-1.amazonaws.com/aroundhome/mr-bot:latest
  allow_failure: true
  before_script: []
  script:
  - "/app/staging2production"
  only:
    refs:
    - staging
  needs: []
  retry:
    max: 2
    when:
    - runner_system_failure
  variables:
    STAGING_BRANCH_NAME: staging
    PRODUCTION_BRANCH_NAME: production
